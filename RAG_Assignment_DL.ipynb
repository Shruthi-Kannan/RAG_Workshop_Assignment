{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2aff746-1cad-4bc7-b2b7-905ccfbfce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAG ASSIGNMENT SETUP: Deep Learning\n",
      "============================================================\n",
      "‚úÖ Found 6 PDF(s) to process for Deep Learning topic.\n",
      " - 2015-lecun.pdf\n",
      " - Deep+Learning+Ian+Goodfellow.pdf\n",
      " - deep-learning-material-dept-ece-ase-blr-1.pdf\n",
      " - DeepLearningBook_RefsByLastFirstNames.pdf\n",
      " - lbdl.pdf\n",
      " - Determinants influencing the entrepreneurial success of MSMEs in emerging economies  a study of Indian women entrepreneurs (1)-checkpoint.pdf\n",
      "\n",
      "üìä SUMMARY: Total pages loaded: 1748\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# RAG WORKSHOP ASSIGNMENT - MODIFIED CODE\n",
    "# Topic: Deep Learning Research Papers\n",
    "# ========================================================================\n",
    "\n",
    "# ========================================================================\n",
    "# PART 1: IMPORTS AND SETUP\n",
    "# ========================================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain Document Loaders & Processing\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector Store and Embeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama # Local LLM via Ollama\n",
    "\n",
    "# RAG Chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 1: DOCUMENT DISCOVERY\n",
    "# ========================================================================\n",
    "data_dir = \"./data\"\n",
    "pdf_files = [str(p) for p in Path(data_dir).rglob(\"*.pdf\") if p.is_file()]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RAG ASSIGNMENT SETUP: Deep Learning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"‚ùå ERROR: No PDFs found in {data_dir}. Please add your 5 Deep Learning PDFs.\")\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(pdf_files)} PDF(s) to process for Deep Learning topic.\")\n",
    "    for f in pdf_files:\n",
    "        print(f\" - {os.path.basename(f)}\")\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 2: DOCUMENT LOADING AND PREPROCESSING\n",
    "# ========================================================================\n",
    "documents = []\n",
    "for file_path in pdf_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "        documents.extend(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä SUMMARY: Total pages loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f134e5-232e-4c35-bdd2-061be1850f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß PART 2: Applying Modified Chunking Strategy (1200/200)\n",
      "‚úÖ Successfully split into 3866 text chunks.\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# PART 2 MODIFICATION: TEXT CHUNKING STRATEGY\n",
    "# Chosen Settings for Technical Papers: Larger chunks and overlap.\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\nüîß PART 2: Applying Modified Chunking Strategy (1200/200)\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # --- MODIFIED SETTINGS START ---\n",
    "    chunk_size=1200,       # Increased from 800 for better context retention in research papers\n",
    "    chunk_overlap=200,     # Increased from 150 to reduce information loss between chunks\n",
    "    # --- MODIFIED SETTINGS END ---\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    text.metadata[\"chunk_id\"] = i\n",
    "    text.metadata[\"chunk_length\"] = len(text.page_content)\n",
    "\n",
    "print(f\"‚úÖ Successfully split into {len(texts)} text chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8065627e-3a0b-490d-b42b-31508a843a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Initializing Embedding Model and Vector Store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shruthi Kannan\\AppData\\Local\\Temp\\ipykernel_22500\\2458672110.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created/updated in ./chroma_deep_learning_db with 7732 chunks.\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 4: EMBEDDINGS AND VECTOR STORE\n",
    "# ========================================================================\n",
    "print(\"\\nüß† Initializing Embedding Model and Vector Store...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")\n",
    "\n",
    "db_path = \"./chroma_deep_learning_db\" \n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=db_path\n",
    ")\n",
    "print(f\"‚úÖ Vector store created/updated in {db_path} with {vectorstore._collection.count()} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f2fff3-89ad-4f84-b6dd-62296ba635f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç PART 3: Applying Modified Retrieval Configuration (MMR, k=7, lambda=0.3)\n",
      "   - Documents returned (k): 7\n",
      "   - Relevance vs Diversity balance (lambda_mult): 0.3\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# PART 3 MODIFICATION: RETRIEVAL CONFIGURATION\n",
    "# Chosen Settings: High K/Fetch_K, high diversity (low lambda_mult).\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\nüîç PART 3: Applying Modified Retrieval Configuration (MMR, k=7, lambda=0.3)\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        # --- MODIFIED SETTINGS START ---\n",
    "        \"k\": 7,             # Increased from 5 (More context for LLM)\n",
    "        \"fetch_k\": 15,      # Increased from 10 (Larger pool for MMR selection)\n",
    "        \"lambda_mult\": 0.3  # Decreased from 0.7 (Prioritize diversity, key for synthesis questions)\n",
    "        # --- MODIFIED SETTINGS END ---\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"   - Documents returned (k): {retriever.search_kwargs['k']}\")\n",
    "print(f\"   - Relevance vs Diversity balance (lambda_mult): {retriever.search_kwargs['lambda_mult']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6a8ed9-40d0-4fe3-a02a-c9e8d3f864e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shruthi Kannan\\AppData\\Local\\Temp\\ipykernel_22500\\2164783948.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain assembled successfully!\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# PART 6 & 7: LLM, PROMPT, AND CHAIN SETUP\n",
    "# ========================================================================\n",
    "try:\n",
    "    llm = Ollama(\n",
    "        model=\"phi3:mini\",\n",
    "        temperature=0.2,\n",
    "        num_thread=2,\n",
    "    )\n",
    "    # Test LLM connection\n",
    "    llm.invoke(\"Test response: What is 2+2?\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå LLM Connection Failed: {e}. Ensure Ollama is running.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a precise document analyst. Your task is to answer questions STRICTLY based on the provided context.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. ONLY use information explicitly stated in the context below\n",
    "2. If the context doesn't contain the answer, respond: \"The provided documents do not contain information to answer this question.\"\n",
    "3. Always cite which document/source your answer comes from\n",
    "4. Do not make inferences beyond what is directly stated\n",
    "5. If multiple sources contradict each other, mention the contradiction\n",
    "6. Use exact quotes when possible, enclosed in quotation marks\n",
    "7. For factual questions (like definitions, equations, or parameters), scan ALL context carefully\n",
    "\n",
    "Context Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Requirements for your answer:\n",
    "- Start with the most relevant source\n",
    "- Use direct quotes where applicable\n",
    "- Clearly separate facts from different sources\n",
    "- Look for keywords related to the question (e.g., algorithm, loss function, architecture, parameter)\n",
    "- End with source citations\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PROMPT,\n",
    "        \"document_separator\": \"\\n\\n--- SOURCE DOCUMENT ---\\n\\n\"\n",
    "    },\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain assembled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a53af5d-1845-4fea-8a41-bcd4bc20729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 9: ANSWER VALIDATION SYSTEM\n",
    "# (Keeping original validation functions for assignment requirements)\n",
    "# ========================================================================\n",
    "\n",
    "def validate_answer(answer, source_docs):\n",
    "    answer_lower = answer.lower()\n",
    "    hallucination_phrases = [\n",
    "        \"i think\", \"probably\", \"likely\", \"it seems\", \"perhaps\", \n",
    "        \"generally speaking\", \"typically\", \"usually\", \"in most cases\"\n",
    "    ]\n",
    "    confidence_score = 1.0\n",
    "    warnings = []\n",
    "    \n",
    "    for phrase in hallucination_phrases:\n",
    "        if phrase in answer_lower:\n",
    "            confidence_score -= 0.2\n",
    "            warnings.append(f\"Uncertain language detected: '{phrase}'\")\n",
    "    \n",
    "    has_citations = any(doc.metadata['source'].lower() in answer_lower for doc in source_docs)\n",
    "    if not has_citations and \"do not contain information\" not in answer_lower:\n",
    "        confidence_score -= 0.3\n",
    "        warnings.append(\"Answer does not explicitly reference source documents\")\n",
    "    \n",
    "    return max(0.0, confidence_score), warnings\n",
    "\n",
    "def ask_question_with_validation(question):\n",
    "    print(f\"\\nü§î Question: {question}\")\n",
    "    print(\"üîç Retrieving relevant information...\")\n",
    "    \n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    answer = result[\"result\"]\n",
    "    source_docs = result[\"source_documents\"]\n",
    "    \n",
    "    confidence, warnings = validate_answer(answer, source_docs)\n",
    "    \n",
    "    print(\"\\nüìù Answer:\")\n",
    "    print(\"=\"*50)\n",
    "    print(answer)\n",
    "    \n",
    "    print(f\"\\nüìä Quality Assessment: Confidence Score: {confidence:.2f}/1.0\")\n",
    "    if warnings:\n",
    "        print(\"‚ö†Ô∏è  Quality Warnings:\")\n",
    "        for warning in warnings:\n",
    "            print(f\"   ‚Ä¢ {warning}\")\n",
    "    \n",
    "    print(f\"\\nüìö Retrieved Sources ({len(source_docs)} documents):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, doc in enumerate(source_docs):\n",
    "        print(f\"{i+1}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return result, confidence, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa5440b-7a39-4824-a99e-b6fb8b6b2898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WORKSHOP ASSIGNMENT DEMO: RUNNING TEST QUERIES\n",
      "================================================================================\n",
      "\n",
      "--- TEST QUESTION 1 (Factual Retrieval - Model Architecture) ---\n",
      "\n",
      "ü§î Question: What are some of the challenges in neural network optimization?\n",
      "üîç Retrieving relevant information...\n",
      "\n",
      "üìù Answer:\n",
      "==================================================\n",
      "One of the primary challenges in neural network optimization is ill-conditioning as mentioned by Blum and Rivest (1992), which can cause \"SGD to get stuck\" because even small steps may significantly increase the cost function. This issue arises due to problems with the Hessian matrix being positive semidefinite, leading to saddle points where local minima are not global ones:\n",
      "> Some theoretical results show that there exist problem classes that are intractable (Judd 1989; Wolpert and Macready 1997). Other results indicate finding a solution for a network of a given size is infeasible, but \"in practice we can find a solution easily by using a larger network\" (Blum and Rivest, 1992:4.9-50; Wolpert and Macready, 1997).\n",
      "\n",
      "Additionally, the nonconvex nature of neural networks presents difficulties as they are not always expressible in terms of convex optimization problems which have stronger guarantees (Boyd and Vandenberghe, 2004:8.6-8.7):\n",
      "> Convex optimization algorithms can provide many more guarantees by making \"stronger restrictions\" but most deep learning problems cannot be expressed as such due to the nonconvex nature of neural networks which lack saddle points (Boyd and Vandenberghe, 2004:8.6-8.7).\n",
      "\n",
      "Lastly, designing models that are easier to optimize is crucial for improving optimization algorithms since \"the best strategy\" often involves model improvements rather than just algorithm enhancements (Chapter 8 of the provided document):\n",
      "> To improve optimization, it's not always about better algorithms. Instead, many advances in neural network learning have come from designing models that are easier to optimize by using activation functions with significant slope and linear transformations between layers which reflect a \"design choice\" for modern deep networks (Chapter 8 of the provided document).\n",
      "\n",
      "References:\n",
      "- Blum, A., & Rivest, R. L. (1992). On choosing good initial points in optimization algorithms. In Proceedings of Neural Information Processing Systems (NIPS) - Volume II.\n",
      "- Boyd, S. and Vandenberghe, R. (2004). Convex Optimization. Cambridge University Press.\n",
      "- Judd, C. A., & Fanget, J. F. (1989). On the convergence of stochastic gradient methods for nonconvex programming problems with linear constraints and quadratic objectives. Journal of Global Optimization, 3(2), 167‚Äì184.\n",
      "- Wolpert, T., & Macready, W. G. (1997). No free lunch: The proof is simple but the consequences are profound. IEEE Transactions on Evolutionary Computation, 1(1), 62‚Äì73.\n",
      "\n",
      "üìä Quality Assessment: Confidence Score: 0.70/1.0\n",
      "‚ö†Ô∏è  Quality Warnings:\n",
      "   ‚Ä¢ Answer does not explicitly reference source documents\n",
      "\n",
      "üìö Retrieved Sources (7 documents):\n",
      "------------------------------------------------------------\n",
      "1. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: within that well-behaved region. This last view suggests research into choosing\n",
      "good initial points for traditional optimization algorithms to use.\n",
      "8.2.8 Theoretical Limits of Optimization\n",
      "Several the...\n",
      "\n",
      "2. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: the objective function and constraints to ensure that the optimization problem is\n",
      "convex. When training neural networks, we must confront the general non-convex\n",
      "case. Even convex optimization is not w...\n",
      "\n",
      "3. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: in practice we can Ô¨Ånd a solution easily by using a larger network for which many\n",
      "more parameter settings correspond to an acceptable solution. Moreover, in the\n",
      "context of neural network training, we ...\n",
      "\n",
      "4. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: optimization in general.\n",
      "This chapter focuses on one particular case of optimization: Ô¨Ånding the param-\n",
      "eters Œ∏ of a neural network that signiÔ¨Åcantly reduce a cost function J(Œ∏), which\n",
      "typically inclu...\n",
      "\n",
      "5. Source: deep-learning-material-dept-ece-ase-blr-1.pdf\n",
      "   Content: yi = ezi\n",
      "‚àëjezj\n",
      "A strong prediction would have a single entry in the vector close to 1, while the\n",
      "remaining entries would be close to 0. A weak prediction would have multiple\n",
      "possible labels that are m...\n",
      "\n",
      "6. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: CHAPTER 4. NUMERICAL COMPUTATION\n",
      "a small change in the output. Lipschitz continuity is also a fairly weak constraint,\n",
      "and many optimization problems in deep learning can be made Lipschitz continuous\n",
      "w...\n",
      "\n",
      "7. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS\n",
      "8.7.5 Designing Models to Aid Optimization\n",
      "Toimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization\n",
      "algorithm. Instead, many improveme...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "--- TEST QUESTION 2 (Synthesis/MMR Test - Loss Functions) ---\n",
      "\n",
      "ü§î Question: Explain Adversarial Training in simple terms.\n",
      "üîç Retrieving relevant information...\n",
      "\n",
      "üìù Answer:\n",
      "==================================================\n",
      "Adversarial Training is a technique used in machine learning that involves training two models simultaneously‚Äîa generator and a discriminator‚Äîin what resembles a zero-sum game. The goal for the generator (g) is to create samples x such that they are indistinguishable from real data by the discriminator, while the discriminator's objective (d) is to accurately distinguish between genuine and generated samples. This process encourages both models to improve their performance through competition.\n",
      "\n",
      "The generator receives a negative payoff when its output causes an error in the discriminator: \"The simplest way to formulate learning in generative adversarial networks is as a zero-sum game, in which a functionv(Œ∏( )g ,Œ∏( )d ) determines the payoÔ¨Ä of the\n",
      "discriminator.\" (Goodfellow et al. 2014b) The discriminator's reward depends on its ability to correctly classify samples as real or fake: \"The simplest way to formulate learning in generative adversarial networks is as a zero-sum game, where the generator receives ‚àív(Œ∏( )g ,Œ∏( )d ).\" (Goodfellow et al. 2014b)\n",
      "\n",
      "This method has been shown to be effective even when neural network models reach human performance levels on i.i.d. test sets: \"Adversarial examples havemanyimplications, for example, in computersecurity...\" (Szegedy et al. 2014a). However, the technique also reveals that these networks may not truly understand tasks as humans do since they can be easily misled by adversarially crafted inputs: \"Even neural networks that perform at human level accuracy have a nearly 100% error rate on examples...\" (Szegedy et al. 2014a).\n",
      "\n",
      "Source citations include Goodfellow, I., Pixelnervation and Generative Adversarial Networks( , ) as well as Szegedy et al.'s work in Advances in Neural Information Processing Systems( ).\n",
      "\n",
      "üìä Quality Assessment: Confidence Score: 0.70/1.0\n",
      "‚ö†Ô∏è  Quality Warnings:\n",
      "   ‚Ä¢ Answer does not explicitly reference source documents\n",
      "\n",
      "üìö Retrieved Sources (7 documents):\n",
      "------------------------------------------------------------\n",
      "1. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: in the neighborhood of the training data. This can be seen as a way of explicitly\n",
      "introducing a local constancy prior into supervised neural nets.\n",
      "Adversarial training helps to illustrate the power of...\n",
      "\n",
      "2. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: true label. We can seek an adversarial example xÓÄ∞ that causes the classiÔ¨Åer to\n",
      "output a label yÓÄ∞ with yÓÄ∞ ÓÄ∂= ÀÜy. Adversarial examples generated using not the true\n",
      "label but a label provided by a traine...\n",
      "\n",
      "3. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: makes it an excellent manifold learning algorithm. See Ô¨Ågure for examples of20.6\n",
      "low-dimensional manifolds learned by the variational autoencoder. In one of the\n",
      "cases demonstrated in the Ô¨Ågure, the al...\n",
      "\n",
      "4. Source: lbdl.pdf\n",
      "   Content: Chapter 3\n",
      "Training\n",
      "As introduced in ¬ß 1.1, training a model consists\n",
      "of minimizing a loss ‚Ñí(w) which reflects the\n",
      "performance of the predictor f(¬∑;w) on a train train train train train train train tra...\n",
      "\n",
      "5. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: CHAPTER 7. REGULARIZATION FOR DEEP LEARNING\n",
      "+ .007 √ó =\n",
      "x sign(‚àáxJ(Œ∏ x, ,y)) x +\n",
      "ÓÄèsign(‚àáxJ(Œ∏ x, ,y))\n",
      "y =‚Äúpanda‚Äù ‚Äúnematode‚Äù ‚Äúgibbon‚Äù\n",
      "w/ 57.7%\n",
      "conÔ¨Ådence\n",
      "w/ 8.2%\n",
      "conÔ¨Ådence\n",
      "w/ 99.3 %\n",
      "conÔ¨Ådence\n",
      "Figure 7.8: ...\n",
      "\n",
      "6. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: dropout unnecessary. Batch normalization is described further in section .8.7.1\n",
      "7.13 Adversarial Training\n",
      "In many cases, neural networks have begun to reach human performance when\n",
      "evaluated on an i.i....\n",
      "\n",
      "7. Source: Deep+Learning+Ian+Goodfellow.pdf\n",
      "   Content: rather than a fake sample drawn from the model.\n",
      "The simplest way to formulate learning in generative adversarial networks is\n",
      "as a zero-sum game, in which a functionv(Œ∏( )g ,Œ∏( )d ) determines the payo...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üöÄ ASSIGNMENT CODE COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 10: HANDS-ON TESTING - Running Test Questions\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORKSHOP ASSIGNMENT DEMO: RUNNING TEST QUERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# PART 3 Submission Requirement: Test with 2 questions showing it works better\n",
    "print(\"\\n--- TEST QUESTION 1 (Factual Retrieval - Model Architecture) ---\")\n",
    "# Example question for a Deep Learning paper:\n",
    "test_q1 = \"What are some of the challenges in neural network optimization?\" \n",
    "ask_question_with_validation(test_q1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- TEST QUESTION 2 (Synthesis/MMR Test - Loss Functions) ---\")\n",
    "# Example question requiring retrieval from multiple sources (MMR test):\n",
    "test_q2 = \"Explain Adversarial Training in simple terms.\"\n",
    "ask_question_with_validation(test_q2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ ASSIGNMENT CODE COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b726a1-6e79-4e25-ae90-55860339c8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
