{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aff746-1cad-4bc7-b2b7-905ccfbfce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# RAG WORKSHOP ASSIGNMENT - MODIFIED CODE\n",
    "# Topic: Deep Learning Research Papers\n",
    "# ========================================================================\n",
    "\n",
    "# ========================================================================\n",
    "# PART 1: IMPORTS AND SETUP\n",
    "# ========================================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain Document Loaders & Processing\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector Store and Embeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama # Local LLM via Ollama\n",
    "\n",
    "# RAG Chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 1: DOCUMENT DISCOVERY\n",
    "# ========================================================================\n",
    "data_dir = \"./data\"\n",
    "pdf_files = [str(p) for p in Path(data_dir).rglob(\"*.pdf\") if p.is_file()]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RAG ASSIGNMENT SETUP: Deep Learning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"‚ùå ERROR: No PDFs found in {data_dir}. Please add your 5 Deep Learning PDFs.\")\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(pdf_files)} PDF(s) to process for Deep Learning topic.\")\n",
    "    for f in pdf_files:\n",
    "        print(f\" - {os.path.basename(f)}\")\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 2: DOCUMENT LOADING AND PREPROCESSING\n",
    "# ========================================================================\n",
    "documents = []\n",
    "for file_path in pdf_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "        documents.extend(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä SUMMARY: Total pages loaded: {len(documents)}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART 2 MODIFICATION: TEXT CHUNKING STRATEGY\n",
    "# Chosen Settings for Technical Papers: Larger chunks and overlap.\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\nüîß PART 2: Applying Modified Chunking Strategy (1200/200)\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # --- MODIFIED SETTINGS START ---\n",
    "    chunk_size=1200,       # Increased from 800 for better context retention in research papers\n",
    "    chunk_overlap=200,     # Increased from 150 to reduce information loss between chunks\n",
    "    # --- MODIFIED SETTINGS END ---\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    text.metadata[\"chunk_id\"] = i\n",
    "    text.metadata[\"chunk_length\"] = len(text.page_content)\n",
    "\n",
    "print(f\"‚úÖ Successfully split into {len(texts)} text chunks.\")\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 4: EMBEDDINGS AND VECTOR STORE\n",
    "# ========================================================================\n",
    "print(\"\\nüß† Initializing Embedding Model and Vector Store...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")\n",
    "\n",
    "db_path = \"./chroma_deep_learning_db\" \n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=db_path\n",
    ")\n",
    "print(f\"‚úÖ Vector store created/updated in {db_path} with {vectorstore._collection.count()} chunks.\")\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# PART 3 MODIFICATION: RETRIEVAL CONFIGURATION\n",
    "# Chosen Settings: High K/Fetch_K, high diversity (low lambda_mult).\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\nüîç PART 3: Applying Modified Retrieval Configuration (MMR, k=7, lambda=0.3)\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        # --- MODIFIED SETTINGS START ---\n",
    "        \"k\": 7,             # Increased from 5 (More context for LLM)\n",
    "        \"fetch_k\": 15,      # Increased from 10 (Larger pool for MMR selection)\n",
    "        \"lambda_mult\": 0.3  # Decreased from 0.7 (Prioritize diversity, key for synthesis questions)\n",
    "        # --- MODIFIED SETTINGS END ---\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"   - Documents returned (k): {retriever.search_kwargs['k']}\")\n",
    "print(f\"   - Relevance vs Diversity balance (lambda_mult): {retriever.search_kwargs['lambda_mult']}\")\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# PART 6 & 7: LLM, PROMPT, AND CHAIN SETUP\n",
    "# ========================================================================\n",
    "try:\n",
    "    llm = Ollama(\n",
    "        model=\"phi3:mini\",\n",
    "        temperature=0.2,\n",
    "        num_thread=2,\n",
    "    )\n",
    "    # Test LLM connection\n",
    "    llm.invoke(\"Test response: What is 2+2?\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå LLM Connection Failed: {e}. Ensure Ollama is running.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a precise document analyst. Your task is to answer questions STRICTLY based on the provided context.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. ONLY use information explicitly stated in the context below\n",
    "2. If the context doesn't contain the answer, respond: \"The provided documents do not contain information to answer this question.\"\n",
    "3. Always cite which document/source your answer comes from\n",
    "4. Do not make inferences beyond what is directly stated\n",
    "5. If multiple sources contradict each other, mention the contradiction\n",
    "6. Use exact quotes when possible, enclosed in quotation marks\n",
    "7. For factual questions (like definitions, equations, or parameters), scan ALL context carefully\n",
    "\n",
    "Context Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Requirements for your answer:\n",
    "- Start with the most relevant source\n",
    "- Use direct quotes where applicable\n",
    "- Clearly separate facts from different sources\n",
    "- Look for keywords related to the question (e.g., algorithm, loss function, architecture, parameter)\n",
    "- End with source citations\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PROMPT,\n",
    "        \"document_separator\": \"\\n\\n--- SOURCE DOCUMENT ---\\n\\n\"\n",
    "    },\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain assembled successfully!\")\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 9: ANSWER VALIDATION SYSTEM\n",
    "# (Keeping original validation functions for assignment requirements)\n",
    "# ========================================================================\n",
    "\n",
    "def validate_answer(answer, source_docs):\n",
    "    answer_lower = answer.lower()\n",
    "    hallucination_phrases = [\n",
    "        \"i think\", \"probably\", \"likely\", \"it seems\", \"perhaps\", \n",
    "        \"generally speaking\", \"typically\", \"usually\", \"in most cases\"\n",
    "    ]\n",
    "    confidence_score = 1.0\n",
    "    warnings = []\n",
    "    \n",
    "    for phrase in hallucination_phrases:\n",
    "        if phrase in answer_lower:\n",
    "            confidence_score -= 0.2\n",
    "            warnings.append(f\"Uncertain language detected: '{phrase}'\")\n",
    "    \n",
    "    has_citations = any(doc.metadata['source'].lower() in answer_lower for doc in source_docs)\n",
    "    if not has_citations and \"do not contain information\" not in answer_lower:\n",
    "        confidence_score -= 0.3\n",
    "        warnings.append(\"Answer does not explicitly reference source documents\")\n",
    "    \n",
    "    return max(0.0, confidence_score), warnings\n",
    "\n",
    "def ask_question_with_validation(question):\n",
    "    print(f\"\\nü§î Question: {question}\")\n",
    "    print(\"üîç Retrieving relevant information...\")\n",
    "    \n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    answer = result[\"result\"]\n",
    "    source_docs = result[\"source_documents\"]\n",
    "    \n",
    "    confidence, warnings = validate_answer(answer, source_docs)\n",
    "    \n",
    "    print(\"\\nüìù Answer:\")\n",
    "    print(\"=\"*50)\n",
    "    print(answer)\n",
    "    \n",
    "    print(f\"\\nüìä Quality Assessment: Confidence Score: {confidence:.2f}/1.0\")\n",
    "    if warnings:\n",
    "        print(\"‚ö†Ô∏è  Quality Warnings:\")\n",
    "        for warning in warnings:\n",
    "            print(f\"   ‚Ä¢ {warning}\")\n",
    "    \n",
    "    print(f\"\\nüìö Retrieved Sources ({len(source_docs)} documents):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, doc in enumerate(source_docs):\n",
    "        print(f\"{i+1}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return result, confidence, warnings\n",
    "\n",
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 10: HANDS-ON TESTING - Running Test Questions\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORKSHOP ASSIGNMENT DEMO: RUNNING TEST QUERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# PART 3 Submission Requirement: Test with 2 questions showing it works better\n",
    "print(\"\\n--- TEST QUESTION 1 (Factual Retrieval - Model Architecture) ---\")\n",
    "# Example question for a Deep Learning paper:\n",
    "test_q1 = \"What are the main components of the attention mechanism described in the Transformer paper?\" \n",
    "ask_question_with_validation(test_q1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- TEST QUESTION 2 (Synthesis/MMR Test - Loss Functions) ---\")\n",
    "# Example question requiring retrieval from multiple sources (MMR test):\n",
    "test_q2 = \"Compare and contrast the Cross-Entropy Loss and the Mean Squared Error loss functions based on their typical use cases in the provided papers.\"\n",
    "ask_question_with_validation(test_q2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ ASSIGNMENT CODE COMPLETE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
